# ðŸ§  Module 2 LLM Version: Memory Systems - Lesson 3 ðŸ”

![Retrieval Patterns](https://media.giphy.com/media/l0HlQXlQ3nHyLMvte/giphy.gif)

## ðŸŽ¯ Lesson Objectives

By the end of this lesson, you will:
- ðŸ” Master **retrieval patterns** for contextual memory with LLM enhancement
- ðŸ§© Implement **context-aware retrieval** using conversation history
- ðŸ”„ Create **query enhancement** systems with real LLMs
- ðŸ§  Build a **retrieval agent** that understands user intent
- ðŸ”® Develop **explainable retrieval** systems that show their reasoning

---

## ðŸ“š Introduction to Retrieval Patterns with LLM Enhancement

![Retrieval](https://media.giphy.com/media/3o7TKsQ8Xb3gcGEgZW/giphy.gif)

Retrieval patterns are strategies for finding and retrieving relevant information from memory systems. In this lesson, we'll explore how to enhance these patterns with real LLM integration using the Groq API, enabling much more sophisticated and context-aware retrieval.

> ðŸ’¡ **Note on LLM Integration**: Unlike the standard Module 2 which uses simulated retrieval, this version uses real LLMs to enhance queries, understand context, and improve retrieval results. This allows for much more nuanced and effective information retrieval.

### Why Retrieval Patterns Matter

Effective retrieval patterns are crucial for AI agents because:
- They determine what information the agent has access to when making decisions
- They influence the relevance and quality of the agent's responses
- They enable the agent to maintain context across multiple interactions
- They allow the agent to leverage its knowledge base effectively

With LLM enhancement, these retrieval patterns become even more powerful, as the LLM can understand context, infer relationships, and generate more effective queries.

---

## ðŸ§© Context-Aware Retrieval with LLM Enhancement

![Context](https://media.giphy.com/media/3o7TKsQ8Xb3gcGEgZW/giphy.gif)

Context-aware retrieval uses the conversation history to improve the relevance of retrieved information. Let's look at how to implement this with LLM enhancement:

```python
def retrieve_with_context(self, query, top_k=5):
    """Retrieve knowledge with conversation context for better relevance"""
    # Get recent conversation turns
    recent_turns = self.conversation_memory.get_recent(5)
    
    # Extract conversation context using LLM
    conversation_context = self._extract_conversation_context(recent_turns)
    
    # Enhance query with context using LLM
    enhanced_query = self._enhance_query_with_context(query, conversation_context)
    
    # Retrieve with enhanced query
    results = self.vector_db.search_with_expansion(enhanced_query, top_k=top_k)
    
    # Add context information to results
    for result in results:
        result['context_info'] = {
            'original_query': query,
            'enhanced_query': enhanced_query,
            'conversation_context': conversation_context
        }
    
    return results
```

This function:
1. Retrieves recent conversation turns from memory
2. Uses the LLM to extract context from the conversation
3. Uses the LLM to enhance the query based on this context
4. Retrieves information using the enhanced query
5. Adds context information to the results for transparency

### Extracting Conversation Context with LLM

The key to context-aware retrieval is extracting meaningful context from the conversation history:

```python
def _extract_conversation_context(self, conversation_turns):
    """Extract context from conversation history using LLM"""
    if not conversation_turns:
        return {
            'topics': [],
            'entities': [],
            'user_interests': [],
            'open_questions': []
        }
    
    # Format conversation for the LLM
    conversation_text = "\n".join([
        f"{turn['content']['role']}: {turn['content']['content']}"
        for turn in conversation_turns
    ])
    
    prompt = f"""
    Analyze this conversation history:
    
    {conversation_text}
    
    Extract the following information:
    - topics: Main topics discussed in the conversation
    - entities: Named entities mentioned (people, organizations, places, etc.)
    - user_interests: What the user seems interested in
    - open_questions: Any questions that haven't been fully answered
    
    Return a JSON object with these fields. Return only the JSON object, without additional text.
    """
    
    response = self.groq_client.generate_text(prompt, max_tokens=300)
    result = self.groq_client.extract_text_from_response(response)
    
    # Parse the JSON response
    try:
        return json.loads(result)
    except json.JSONDecodeError:
        # If the response is not valid JSON, return a simplified structure
        return {
            'topics': [],
            'entities': [],
            'user_interests': [],
            'open_questions': []
        }
```

This function uses the LLM to analyze the conversation history and extract structured information about topics, entities, user interests, and open questions. This rich context enables much more sophisticated retrieval than simple keyword matching.

---

## ðŸ”„ Query Enhancement with LLM

![Query Enhancement](https://media.giphy.com/media/3o7TKsQ8Xb3gcGEgZW/giphy.gif)

Query enhancement uses the LLM to improve the original query based on context. This is particularly useful for:
- Resolving ambiguities in the query
- Adding implicit context from the conversation
- Expanding the query with related terms
- Focusing the query on the most relevant aspects

Let's look at how to implement query enhancement with LLM:

```python
def _enhance_query_with_context(self, query, context):
    """Enhance a query with conversation context using LLM"""
    # Format context for the LLM
    context_text = json.dumps(context, indent=2)
    
    prompt = f"""
    Original query: "{query}"
    
    Conversation context:
    {context_text}
    
    Enhance the original query to make it more specific and relevant based on the conversation context.
    The enhanced query should help retrieve more relevant information.
    
    Return only the enhanced query, without quotes or additional text.
    """
    
    response = self.groq_client.generate_text(prompt, max_tokens=150)
    enhanced_query = self.groq_client.extract_text_from_response(response).strip()
    
    # If the enhancement failed or returned nothing, use the original query
    if not enhanced_query:
        return query
    
    return enhanced_query
```

This function uses the LLM to generate an enhanced version of the query that incorporates the conversation context. For example, if the original query is "What are the best tools?" and the conversation has been about Python and machine learning, the enhanced query might be "What are the best Python libraries for machine learning?".

### Example of Query Enhancement

Consider this conversation:

```
User: What is Python?
Agent: Python is a high-level programming language known for its readability and simplicity...

User: What is it used for?
```

Without context, the second query "What is it used for?" is ambiguous. But with query enhancement, it becomes "What is Python used for?", which is much more specific and can retrieve more relevant information.

---

## ðŸ§  Building a Retrieval Agent with LLM Integration

![Retrieval Agent](https://media.giphy.com/media/3o7TKsQ8Xb3gcGEgZW/giphy.gif)

A retrieval agent combines all these patterns into a cohesive system that can:
1. Process user input and extract key information
2. Maintain conversation context
3. Enhance queries based on context
4. Retrieve relevant information
5. Generate responses based on retrieved information

Let's look at the key components of our retrieval agent:

### Processing User Input with LLM

```python
def process_user_input(self, user_input):
    """Process user input and update conversation memory"""
    # Add to conversation memory
    self.conversation_memory.add({
        'role': 'user',
        'content': user_input,
        'timestamp': datetime.now().isoformat()
    })
    
    # Extract key information using LLM
    key_info = self._extract_key_information(user_input)
    
    return {
        'text': user_input,
        'key_information': key_info,
        'timestamp': datetime.now().isoformat()
    }
```

This function adds the user input to conversation memory and uses the LLM to extract key information like topics, entities, intent, and sentiment.

### Generating Responses with LLM

```python
def generate_response(self, query, retrieved_items):
    """Generate a response based on retrieved items using LLM"""
    if not retrieved_items:
        return "I don't have any information about that."
    
    # Format retrieved items for the LLM
    retrieved_text = "\n\n".join([
        f"Item {i+1}:\n{item['text']}"
        for i, item in enumerate(retrieved_items)
    ])
    
    # Get recent conversation for context
    recent_turns = self.conversation_memory.get_recent(3)
    conversation_text = "\n".join([
        f"{turn['content']['role']}: {turn['content']['content']}"
        for turn in recent_turns
    ])
    
    prompt = f"""
    User query: "{query}"
    
    Recent conversation:
    {conversation_text}
    
    Retrieved information:
    {retrieved_text}
    
    Based on the user's query and the retrieved information, generate a helpful and informative response.
    If the retrieved information doesn't fully answer the query, acknowledge that.
    If there are multiple relevant pieces of information, synthesize them into a coherent response.
    
    Response:
    """
    
    response = self.groq_client.generate_text(prompt, max_tokens=500)
    return self.groq_client.extract_text_from_response(response)
```

This function uses the LLM to generate a response based on the retrieved information and conversation context. The LLM can synthesize information from multiple sources, acknowledge limitations, and generate a coherent response.

### Putting It All Together

The `answer_question` method ties everything together:

```python
def answer_question(self, question):
    """Answer a question using the retrieval agent"""
    # Process the question
    processed_input = self.process_user_input(question)
    
    # Retrieve relevant knowledge with context
    retrieved_items = self.retrieve_with_context(question, top_k=5)
    
    # Generate a response
    response = self.generate_response(question, retrieved_items)
    
    # Store the response
    self.store_response(response)
    
    return response
```

This method:
1. Processes the user's question and extracts key information
2. Retrieves relevant knowledge using context-aware retrieval
3. Generates a response based on the retrieved information
4. Stores the response in conversation memory for future context

---

## ðŸ”® Explainable Retrieval with LLM

![Explainable Retrieval](https://media.giphy.com/media/3o7TKsQ8Xb3gcGEgZW/giphy.gif)

Explainable retrieval helps users understand how the system found information. This is particularly important for building trust and helping users refine their queries.

Let's look at how to implement explainable retrieval with LLM:

```python
def explain_retrieval(self, query):
    """Explain the retrieval process for a query"""
    # Process the query
    processed_input = self.process_user_input(query)
    
    # Get conversation context
    recent_turns = self.conversation_memory.get_recent(5)
    conversation_context = self._extract_conversation_context(recent_turns)
    
    # Enhance query with context
    enhanced_query = self._enhance_query_with_context(query, conversation_context)
    
    # Expand the enhanced query
    expanded_queries = self.vector_db.expand_query(enhanced_query)
    
    # Retrieve with each expanded query
    all_results = []
    for expanded_query in expanded_queries:
        results = self.vector_db.search(expanded_query, top_k=3)
        for result in results:
            result['expanded_query'] = expanded_query
        all_results.extend(results)
    
    # Remove duplicates and sort
    seen_ids = set()
    unique_results = []
    for result in all_results:
        if result['id'] not in seen_ids:
            seen_ids.add(result['id'])
            unique_results.append(result)
    
    unique_results.sort(key=lambda x: x['similarity'], reverse=True)
    top_results = unique_results[:5]
    
    # Generate explanation
    explanation = self._generate_retrieval_explanation(
        query, 
        enhanced_query, 
        expanded_queries, 
        top_results, 
        conversation_context
    )
    
    return {
        'original_query': query,
        'enhanced_query': enhanced_query,
        'expanded_queries': expanded_queries,
        'conversation_context': conversation_context,
        'top_results': top_results,
        'explanation': explanation
    }
```

This function:
1. Processes the query and extracts conversation context
2. Enhances the query based on context
3. Expands the enhanced query for better recall
4. Retrieves information using the expanded queries
5. Generates an explanation of the retrieval process

### Generating Retrieval Explanations with LLM

The key to explainable retrieval is generating clear explanations of the retrieval process:

```python
def _generate_retrieval_explanation(self, original_query, enhanced_query, expanded_queries, results, context):
    """Generate an explanation of the retrieval process using LLM"""
    # Format inputs for the LLM
    expanded_text = "\n".join([f"- {q}" for q in expanded_queries])
    
    results_text = "\n\n".join([
        f"Result {i+1} (Similarity: {result['similarity']:.4f}):\n{result['text'][:100]}..."
        for i, result in enumerate(results)
    ])
    
    context_text = json.dumps(context, indent=2)
    
    prompt = f"""
    Explain the retrieval process for this query:
    
    Original query: "{original_query}"
    
    Step 1: The query was enhanced with conversation context:
    Enhanced query: "{enhanced_query}"
    
    Step 2: The enhanced query was expanded to improve recall:
    {expanded_text}
    
    Step 3: The system searched for relevant information using these queries and found:
    {results_text}
    
    Conversation context used:
    {context_text}
    
    Provide a clear explanation of how the retrieval process worked, including:
    - How the conversation context influenced the query enhancement
    - How query expansion helped find relevant information
    - Why the top results were selected
    - Any challenges or limitations in the retrieval process
    
    Explanation:
    """
    
    response = self.groq_client.generate_text(prompt, max_tokens=500)
    return self.groq_client.extract_text_from_response(response)
```

This function uses the LLM to generate a clear explanation of the retrieval process, including how the conversation context influenced the query, how query expansion helped find relevant information, and why certain results were selected.

---

## ðŸ’ª Practice Exercises

![Practice](https://media.giphy.com/media/3oKIPrc2ngFZ6BTyww/giphy.gif)

1. **Implement a Multi-Strategy Retrieval System**:
   - Create a system that uses different retrieval strategies based on the query type
   - Use the LLM to determine which strategy to use for each query
   - Implement strategies for factual queries, opinion queries, and procedural queries

2. **Build a Personalized Retrieval System**:
   - Extend the retrieval agent to maintain user profiles
   - Use the LLM to update user profiles based on interactions
   - Implement personalized query enhancement based on user profiles

3. **Create a Feedback-Based Retrieval System**:
   - Implement a system that collects feedback on retrieval results
   - Use the LLM to analyze feedback and improve future retrievals
   - Create a mechanism for the system to learn from its mistakes

---

## ðŸŽ¯ Mini-Project Progress: Knowledge Base Assistant with Groq

![Knowledge Base](https://media.giphy.com/media/3o7btPCcdNniyf0ArS/giphy.gif)

In this lesson, we've learned how to implement retrieval patterns with LLM enhancement. These components will be crucial for our Knowledge Base Assistant:

- **Context-Aware Retrieval**: Will help the assistant understand the conversation context
- **Query Enhancement**: Will improve the relevance of retrieved information
- **Response Generation**: Will create coherent responses based on retrieved information
- **Explainable Retrieval**: Will help users understand how the assistant found information

In the next lesson, we'll bring everything together to build the complete Knowledge Base Assistant with Groq integration.

---

## ðŸ“š Resources

- [Groq API Documentation](https://console.groq.com/docs/quickstart)
- [Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2005.11401)
- [LangChain Retrieval](https://python.langchain.com/docs/modules/data_connection/)
- [Explainable AI Techniques](https://christophm.github.io/interpretable-ml-book/)
- [Context-Aware NLP Systems](https://aclanthology.org/2020.acl-main.740/)
